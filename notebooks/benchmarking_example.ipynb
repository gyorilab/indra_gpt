{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract statements for the following configurations\n",
    "1) Common config: {user_inputs_file: None, num_samples: 500, random_sample: True, random_seed: 42, model: gpt-4o-mini, grounding: True}\n",
    "2) Hyperparameters: model, structured_output, n_shot_prompting, self_correction_iterations, grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "# get current path\n",
    "sys.path.append(str(Path.cwd().parent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "from pathlib import Path\n",
    "\n",
    "today_date = strftime(\"%Y-%m-%d\")\n",
    "common_config = {\n",
    "    \"user_inputs_file\": None,\n",
    "    \"num_samples\": 500,\n",
    "    \"random_sample\": True,\n",
    "    \"random_seed\": 42,\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"grounding\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\n",
    "    \"structured_output\": False,\n",
    "    \"n_shot_prompting\": 0,\n",
    "    \"self_correction_iterations\": 0\n",
    "}\n",
    "config_2 = {\n",
    "    \"structured_output\": False,\n",
    "    \"n_shot_prompting\": 0,\n",
    "    \"self_correction_iterations\": 2\n",
    "}\n",
    "config_3 = {\n",
    "    \"structured_output\": False,\n",
    "    \"n_shot_prompting\": 2,\n",
    "    \"self_correction_iterations\": 0\n",
    "}\n",
    "config_4 = {\n",
    "    \"structured_output\": False,\n",
    "    \"n_shot_prompting\": 2,\n",
    "    \"self_correction_iterations\": 2\n",
    "}\n",
    "config_5 = {\n",
    "    \"structured_output\": True,\n",
    "    \"n_shot_prompting\": 0,\n",
    "    \"self_correction_iterations\": 0\n",
    "}\n",
    "config_6 = {\n",
    "    \"structured_output\": True,\n",
    "    \"n_shot_prompting\": 0,\n",
    "    \"self_correction_iterations\": 2\n",
    "}\n",
    "config_7 = {\n",
    "    \"structured_output\": True,\n",
    "    \"n_shot_prompting\": 2,\n",
    "    \"self_correction_iterations\": 0\n",
    "}\n",
    "config_8 = {\n",
    "    \"structured_output\": True,\n",
    "    \"n_shot_prompting\": 2,\n",
    "    \"self_correction_iterations\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name(common_config, specific_config, today_date_str):\n",
    "    formatted_output_file_name = (\n",
    "        f\"stmt_extractions_model_{common_config['model']}_\"\n",
    "        f\"structured-output_{specific_config['structured_output']}_\"\n",
    "        f\"shot-prompting_{specific_config['n_shot_prompting']}_\"\n",
    "        f\"self-corrections_{specific_config['self_correction_iterations']}_\"\n",
    "        f\"{today_date_str}.pkl\"\n",
    "    )\n",
    "    return formatted_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [config_1, config_2, config_3, config_4, config_5, config_6, config_7, config_8]\n",
    "for config in configs:\n",
    "    output_file_name = get_output_file_name(common_config, config, today_date)\n",
    "    config[\"output_file\"] = output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configs:\n",
    "    config.update(common_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'structured_output': False, 'n_shot_prompting': 0, 'self_correction_iterations': 0, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_False_shot-prompting_0_self-corrections_0_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': False, 'n_shot_prompting': 0, 'self_correction_iterations': 2, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_False_shot-prompting_0_self-corrections_2_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': False, 'n_shot_prompting': 2, 'self_correction_iterations': 0, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_False_shot-prompting_2_self-corrections_0_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': False, 'n_shot_prompting': 2, 'self_correction_iterations': 2, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_False_shot-prompting_2_self-corrections_2_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': True, 'n_shot_prompting': 0, 'self_correction_iterations': 0, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_True_shot-prompting_0_self-corrections_0_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': True, 'n_shot_prompting': 0, 'self_correction_iterations': 2, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_True_shot-prompting_0_self-corrections_2_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': True, 'n_shot_prompting': 2, 'self_correction_iterations': 0, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_True_shot-prompting_2_self-corrections_0_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n",
      "{'structured_output': True, 'n_shot_prompting': 2, 'self_correction_iterations': 2, 'output_file': 'stmt_extractions_model_gpt-4o-mini_structured-output_True_shot-prompting_2_self-corrections_2_2025-03-03.pkl', 'user_inputs_file': None, 'num_samples': 5, 'random_sample': True, 'random_seed': 42, 'model': 'gpt-4o-mini', 'grounding': True}\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [2025-03-03 22:08:00] indra.preassembler.grounding_mapper.disambiguate - INDRA DB is not available for text content retrieval for grounding disambiguation.\n",
      "INFO: [2025-03-03 22:08:00] indra_gpt.scripts.run_statement_json_extraction - Running structured knowledge extraction pipeline...\n",
      "INFO: [2025-03-03 22:08:00] indra_gpt.pipelines.statement_extraction_pipeline - Starting preprocessing step...\n",
      "INFO: [2025-03-03 22:08:00] indra_gpt.processors.pre_processors - Processing user inputs...\n",
      "INFO: [2025-03-03 22:08:00] indra_gpt.pipelines.statement_extraction_pipeline - Running LLM-based statement extraction...\n",
      "INFO: [2025-03-03 22:08:00] indra_gpt.processors.generators - Config used for generation: {'model': 'gpt-4o-mini', 'structured_output': False, 'self_correction_iterations': 0}\n",
      "Extracting:   0%|          | 0/5 [00:00<?, ?statement/s]INFO: [2025-03-03 22:08:01] httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Extracting:   0%|          | 0/5 [00:00<?, ?statement/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'response_content' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mindra_gpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_statement_json_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configs:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gyorilab/indra_gpt/indra_gpt/scripts/run_statement_json_extraction.py:28\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m output_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_DEFAULT\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning structured knowledge extraction pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_and_save_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gyorilab/indra_gpt/indra_gpt/pipelines/statement_extraction_pipeline.py:59\u001b[0m, in \u001b[0;36mStatementExtractionPipeline.run_and_save_results\u001b[0;34m(self, output_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_and_save_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_file):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    Runs the full pipeline and saves the extracted statements to a file.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     raw_input_data, preprocessed_input_data, extracted_json_stmts, preassembled_stmts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_results(output_file, raw_input_data, preprocessed_input_data, extracted_json_stmts, preassembled_stmts)\n",
      "File \u001b[0;32m~/gyorilab/indra_gpt/indra_gpt/pipelines/statement_extraction_pipeline.py:24\u001b[0m, in \u001b[0;36mStatementExtractionPipeline.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m raw_input_data, preprocessed_input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_processor\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning LLM-based statement extraction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m extracted_json_stmts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_input_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost-processing extracted statements...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m preassembled_stmts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processor\u001b[38;5;241m.\u001b[39mprocess(extracted_json_stmts)\n",
      "File \u001b[0;32m~/gyorilab/indra_gpt/indra_gpt/processors/generators.py:33\u001b[0m, in \u001b[0;36mGenerator.generate\u001b[0;34m(self, preprocessed_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Instantiate the appropriate model client dynamically\u001b[39;00m\n\u001b[1;32m     31\u001b[0m client \u001b[38;5;241m=\u001b[39m MODEL_CLIENTS[model](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m---> 33\u001b[0m extracted_statement_json_objects \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extracted_statement_json_objects\n",
      "File \u001b[0;32m~/gyorilab/indra_gpt/indra_gpt/clients/openai/openai_client.py:136\u001b[0m, in \u001b[0;36mOpenAIClient.generate\u001b[0;34m(self, preprocessed_data)\u001b[0m\n\u001b[1;32m    134\u001b[0m     chat_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chat_prompt(input_text)\n\u001b[1;32m    135\u001b[0m     chat_history \u001b[38;5;241m=\u001b[39m flat_n_shot_history\n\u001b[0;32m--> 136\u001b[0m     response_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     extracted_statement_json_objects\u001b[38;5;241m.\u001b[39mappend(response_content)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extracted_statement_json_objects\n",
      "File \u001b[0;32m~/gyorilab/indra_gpt/indra_gpt/clients/openai/openai_client.py:124\u001b[0m, in \u001b[0;36mOpenAIClient.get_response\u001b[0;34m(self, chat_prompt, chat_history, max_tokens)\u001b[0m\n\u001b[1;32m    121\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Response from OpenAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m     response_content \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent  \u001b[38;5;66;03m# Update latest response\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse_content\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'response_content' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from indra_gpt.scripts.run_statement_json_extraction import main\n",
    "\n",
    "for config in configs:\n",
    "    main(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indra_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
